pylab inline
import torch
from torch.nn.parameter import Parameter
import torch.utils.tensorboard as tb
%load_ext tensorboard
import tempfile
log_dir = tempfile.mkdtemp().replace('\\', '/')
%tensorboard --logdir {log_dir} --reload_interval 1

x = torch.rand([1000, 2])
x_in_circle = ((x ** 2).sum(1) < 1)

def accuracy(pred_label):
  return (pred_label == x_in_circle).float().mean()

def show(pred_label):
  scatter(*x.numpy().T, c=pred_label.numpy())
  axis('equal')

def loss(prediction):
  return -(x_in_circle.float()     * (prediction+1e-10).log() +
           (1-x_in_circle.float()) * (1-prediction+1e-10).log()).mean()

class Linear(torch.nn.Module):
  def __init__(self, input_dim):
    super().__init__()
    self.w = Parameter(torch.ones(input_dim))
    self.b = Parameter(-torch.ones(1))

  def forward(self, x):
    return (x * self.w[None,:]).sum(1) + self.b

class LinearClassifier(torch.nn.Module):
  def __init__(self, input_dim):
    super().__init__()
    self.linear = Linear(input_dim)

  def forward(self, x):
    logit = self.linear(x)
    return 1 / (1+ (-logit).exp())

show(x_in_circle)


# TODO: Setup logging
logger = tb.SummaryWriter(log_dir+'/linear', flush_secs=1)
# TODO: Setup the model
classifier = LinearClassifier(2)

learning_rate = 0.5

for iteration in range(5000):
  # TODO: Predicting the labels (and probabilities)
  p_y = classifier(x)
  pred_y = p_y > 0.5

  # TODO: Compute the loss
  l = loss(p_y)

  # TODO: Logging
  logger.add_scalar('loss', l, global_step=iteration)
  logger.add_scalar('accuracy', accuracy(pred_y), global_step=iteration)

  if iteration % 10 == 0:
    fig = figure()
    show(pred_y)
    logger.add_figure('pred_y', fig, global_step=iteration)
    del fig

  # TODO: Compute the gradient
  l.backward()

  # TODO: Update weight using gradient descent
  for p in classifier.parameters():
    p.data[:] -= learning_rate * p.grad
    p.grad.zero_()
  

show(pred_y)


class NonLinearClassifier(torch.nn.Module):
  def __init__(self, input_dim):
    super().__init__()
    self.linear1 = torch.nn.Linear(input_dim, 100)
    torch.nn.init.normal_(self.linear1.weight, std=0.01)
    torch.nn.init.normal_(self.linear1.bias, std=0.01)
    self.linear2 = Linear(100)

  def forward(self, x):
    logit = self.linear2(torch.relu(self.linear1(x)))
    return 1 / (1+ (-logit).exp())

classifier = NonLinearClassifier(2)


# TODO: Setup logging
logger = tb.SummaryWriter(log_dir+'/nonlinear', flush_secs=1)
# TODO: Setup the model
classifier = NonLinearClassifier(2)

learning_rate = 0.5

for iteration in range(5000):
  # TODO: Predicting the labels (and probabilities)
  p_y = classifier(x)
  pred_y = p_y > 0.5

  # TODO: Compute the loss
  l = loss(p_y)

  # TODO: Logging
  logger.add_scalar('loss', l, global_step=iteration)
  logger.add_scalar('accuracy', accuracy(pred_y), global_step=iteration)

  if iteration % 10 == 0:
    fig = figure()
    show(pred_y)
    logger.add_figure('pred_y', fig, global_step=iteration)
    del fig

  # TODO: Compute the gradient
  l.backward()

  # TODO: Update weight using gradient descent
  for p in classifier.parameters():
    p.data[:] -= learning_rate * p.grad
    p.grad.zero_()
  

show(pred_y)
